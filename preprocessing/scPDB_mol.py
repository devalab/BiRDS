# %%
# Some constants that will be required
# ALWAYS RUN THIS CODE CELL
import os

data_dir = os.path.abspath("../data/scPDB")
raw_dir = os.path.join(data_dir, "raw")
pssm_dir = os.path.join(data_dir, "pssm")
splits_dir = os.path.join(data_dir, "splits")
preprocessed_dir = os.path.join(data_dir, "preprocessed")


# %%
# Download the scPDB dataset and extract the dataset in "data/scPDB/raw"
# resource = "http://bioinfo-pharma.u-strasbg.fr/scPDB/ressources/2016/scPDB.tar.gz"
# !aria2c -c -x 8 -s 8 -d $data_dir $resource --out 'scPDB.tar.gz'
# !tar xvzf ../data/scPDB/scPDB.tar.gz -C ../data/scPDB/raw/


# %%
# For 10-fold Cross Validation, we will use the splits that were generated by https://arxiv.org/abs/1904.06517
# resource = "https://gitlab.com/cheminfIBB/kalasanty/-/archive/master/kalasanty-master.tar.gz?path=data"
# !aria2c -c -x 8 -s 8 -d "../data/scPDB" $resource --out 'kalasanty-master-data.tar.gz'
# !tar xvzf ../data/scPDB/kalasanty-master-data.tar.gz -C ../data/scPDB/


# %%
# For sequence-based prediction, we need to use RCSB FASTA files
import urllib.request

for pdb_id_struct in sorted(os.listdir(raw_dir)):
    pdb_id = pdb_id_struct[:4]
    print(pdb_id)

    fasta_save = os.path.join(raw_dir, pdb_id_struct, "sequence.fasta")
    if not os.path.exists(fasta_save):
        try:
            urllib.request.urlretrieve(
                "https://www.rcsb.org/pdb/download/downloadFastaFiles.do?structureIdList="
                + pdb_id
                + "&compressionType=uncompressed",
                fasta_save,
            )
        except:  # noqa: E722
            print("Err: fasta " + pdb_id)


# %%
# Check whether downloaded sequence files are correct
# A few of the PDB files have been obseleted and hence will have to manually download them
# I wrote down the manual mapping of the pdb id's but seem to have lost it
# But they can easily be found out from data/obseleted.txt file

for pdb_id_struct in sorted(os.listdir(raw_dir)):
    pdb_id = pdb_id_struct[:4]
    fasta_save = os.path.join(raw_dir, pdb_id_struct, "sequence.fasta")
    with open(fasta_save, "r") as f:
        line = f.readline()
        if line[0] != ">":
            print("Err: FASTA " + pdb_id_struct)


# %%
# Let us reindex protein.mol2 and site.mol2 and convert to PDB format
# So that they can be imported using biopython
# Also let us write fasta sequence for the chains in protein.mol2 files
# Download NWalign.py to use Needlman Wunsch algorithm to align sequences
# https://zhanglab.ccmb.med.umich.edu/NW-align/NWalign.py (Make small changes for Python3 compatibility)
# 4egb_5 is an obseleted PDB and hence has chain E, had to manually add E to sequence.fasta
from preprocessing.utils.mol2 import Mol2

for pdb_id_struct in sorted(os.listdir(raw_dir)):
    pre = os.path.join(raw_dir, pdb_id_struct)
    sequence_fasta = os.path.join(pre, "sequence.fasta")
    reindexed_prot = os.path.join(pre, "reindexed_protein.pdb")
    reindexed_site = os.path.join(pre, "reindexed_site.pdb")
    if os.path.exists(reindexed_site):
        continue
    print(pdb_id_struct)
    # if pdb_id_struct == "1a5s_1":
    #     exit(1)

    # Import mol2 and reindex the protein according to sequence fasta
    prot_mol = Mol2(os.path.join(pre, "protein.mol2"))
    site_mol = Mol2(os.path.join(pre, "site.mol2"))
    prot_mol.reindex(sequence_fasta)

    # Reindex site.mol2 file according to the alignment from protein.mol2
    site_mol.subst_df["reindex_id"] = 0
    for i, record in enumerate(site_mol.subst_df[["subst_name", "chain"]].values):
        reindex_id = prot_mol.subst_df[
            (prot_mol.subst_df["subst_name"] == record[0])
            & (prot_mol.subst_df["chain"] == record[1])
        ]["reindex_id"]
        # Typically it should not be empty, but very minor error in dataset I think
        if not reindex_id.empty:
            site_mol.subst_df.at[i, "reindex_id"] = reindex_id

    # Write back the reindexed files as pdb
    prot_mol.write_pdb(reindexed_prot)
    site_mol.write_pdb(reindexed_site)

    # We need to generate MSA's, hence store only the fasta of chains
    # from the above protein
    with open(sequence_fasta, "r") as f:
        header = f.readline()
        while 1:
            chain_id = header[6:7]
            sequence = ""
            line = f.readline()
            while line != "" and line is not None and line[0] != ">":
                sequence += line.strip()
                line = f.readline()
            # No need to replace here because U and O will be considered X automatically while making features
            # sequence = sequence.replace("U", "X").replace("O", "X")
            if chain_id in prot_mol.sequences:
                with open(os.path.join(pre, chain_id + ".fasta"), "w") as hlp:
                    hlp.write(header)
                    hlp.write(sequence + "\n")
            if line == "" or line is None:
                break
            header = line


# %%
# In case you want to delete the generated fasta and reindexed files from the above cell, use this
# ! find ../data/scPDB/raw/ -name "?.fasta" -delete
# ! find ../data/scPDB/raw/ -name "reindexed_*" -delete


# %%
# The fasta files generated will have a lot of common sequences
# To speed up MSA generation, let us create a unique file that has common sequences
# Then we can generate the MSAs for only the first chain in every line
# from collections import defaultdict

# sequences = defaultdict(list)
# for file in sorted(os.listdir(raw_dir)):
#     pre = os.path.join(raw_dir, file.strip())
#     for fasta in sorted(os.listdir(pre)):
#         if fasta[2:] != "fasta":
#             continue
#         chain_id = fasta[0]
#         with open(os.path.join(pre, fasta)) as f:
#             f.readline()
#             sequence = f.readline().strip()
#             # This choice was made so that rsync would work much better and easier
#             sequences[sequence].append(file + "/" + chain_id + "*")

# keys = list(sequences.keys())

# with open(os.path.join(data_dir, "unique"), "w") as f:
#     for key in keys:
#         line = ""
#         for chain_id in sequences[key]:
#             line += chain_id + " "
#         f.write(line[:-1] + "\n")


# %%
# MSAs need to be generated for the fasta files
# Refer to https://github.com/crvineeth97/msa-generator


# %%
# MAKING OF .npz FILES FROM HERE
# Let us preprocess ALL the available data and create .npz files which contains the required information of each chain
import warnings

import numpy as np

from Bio import BiopythonWarning
from Bio.PDB import PDBParser, is_aa
from rdkit import Chem
from rdkit.Chem.rdMolTransforms import ComputeCentroid

warnings.simplefilter("ignore", BiopythonWarning)
parser = PDBParser()


def initialize_protein_info(pdb_id_struct, name):
    """
    pdb_id_struct: 10mh_1 from scPDB
    name: Whether reindexed_site.pdb or reindexed_protein.pdb

    Returns: protein = {"structure": ..., "chainA": {"residues": ..., "seqeunce": ...}, ...}
    """
    pre = os.path.join(raw_dir, pdb_id_struct)
    protein = {}
    protein["structure"] = parser.get_structure(pdb_id_struct, os.path.join(pre, name))

    for chain in protein["structure"][0]:
        chain_id = chain.get_id()
        protein[chain_id] = {}
        protein[chain_id]["residues"] = []
        for res in chain:
            id = res.get_id()
            # Include all types of residues, not only the standard 20
            if is_aa(res, standard=False) and id[0] == " ":
                protein[chain_id]["residues"].append(res)

        # Stores the RCSB sequence of the protein, not the sequence from protein
        protein[chain_id]["sequence"] = ""
        with open(os.path.join(pre, chain_id + ".fasta")) as f:
            line = f.readline()
            line = f.readline()
            while line != "" and line is not None:
                protein[chain_id]["sequence"] += line.strip()
                line = f.readline()
    return protein


def initialize_ligand_info(pdb_id_struct):
    """
    pdb_id_struct: 10mh_1 from scPDB

    Returns: ligand = {"supplier": ..., "coords": ..., "num_atoms": ..., "atom_types": ...}
    """
    pre = os.path.join(raw_dir, pdb_id_struct)
    ligand = {}
    ligand["supplier"] = Chem.SDMolSupplier(
        os.path.join(pre, "ligand.sdf"), sanitize=False
    )
    assert len(ligand["supplier"]) == 1
    ligand["supplier"] = ligand["supplier"][0]
    assert ligand["supplier"].GetNumConformers() == 1
    ligand["coords"] = ligand["supplier"].GetConformer().GetPositions()
    ligand["num_atoms"] = ligand["supplier"].GetNumAtoms()
    assert ligand["num_atoms"] == len(ligand["coords"])
    ligand["atom_types"] = np.array(
        [atom.GetSymbol() for atom in ligand["supplier"].GetAtoms()]
    )
    return ligand


def find_site_residues(protein, site):
    """
    protein: protein[chain] dictionary from initialize_protein_info
    site: site dictionary from initialize_protein_info

    Returns: A numpy 1D array of shape (L,) where L is length of RCSB sequence
    and 1 represents that the amino acid is in contact with the ligand
    """
    seq_len = len(protein["sequence"])
    labels = np.zeros(seq_len)
    for residue in site["residues"]:
        res_ind = residue.get_id()[1] - 1
        if res_ind >= seq_len:
            # These are X markers mostly
            continue
        labels[res_ind] = 1
    return labels


def get_protein_ligand_dist(protein, ligand):
    """
    protein: protein[chain] dictionary from initialize_protein_info
    ligand: ligand dictionary from initialize_ligand_info

    Returns: A numpy 1D array of shape (L,) where L is length of RCSB sequence
            and the real numbers represent the distance of the ligand centroid
            from the CB (CA) atom of the amino acid (Gly)
    """
    centroid = ComputeCentroid(ligand["supplier"].GetConformer())
    centroid = np.array([centroid.x, centroid.y, centroid.z])
    seq_len = len(protein["sequence"])
    dist = np.full(seq_len, 1e6)
    for residue in protein["residues"]:
        res_ind = residue.get_id()[1] - 1
        if res_ind >= seq_len:
            continue
        if residue.has_id("CB"):
            atom_type = "CB"
        elif residue.has_id("CA"):
            atom_type = "CA"
        else:
            continue
        dist[res_ind] = np.linalg.norm(residue[atom_type].get_coord() - centroid)
    return dist


def get_distance_map_true(protein):
    """
    protein: protein[chain] dictionary from initialize_protein_info

    Returns: A 2D numpy array of shape (L, L) where L is the length of the RCSB sequence
            and the real numbers of row i and column j represents the distance between
            the CB (CA) of i'th amino acid (Gly) and CB (CA) of j'th amino acid (Gly)
    """
    seq_len = len(protein["sequence"])
    # Might be different from seq_len because of missing residues
    num_residues = len(protein["residues"])
    # Don't use np.inf, use an impossibly large number
    distance_map = np.full((seq_len, seq_len), 1e6)  # Initialize to infinite distance
    for ind1 in range(num_residues):
        at1 = "CB"
        res1 = protein["residues"][ind1]
        if not res1.has_id("CB"):
            at1 = "CA"
            if not res1.has_id("CA"):
                continue
        res1_ind = res1.get_id()[1] - 1
        # There might be some X at the end of the fasta and might have been written to reindexed pdb
        if res1_ind >= seq_len:
            print("Ignored residue", res1.get_resname())
            continue
        for ind2 in range(ind1 + 1, num_residues):
            at2 = "CB"
            res2 = protein["residues"][ind2]
            if not res2.has_id("CB"):
                at2 = "CA"
                if not res2.has_id("CA"):
                    continue
            res2_ind = res2.get_id()[1] - 1
            if res2_ind >= seq_len:
                continue
            dist = np.linalg.norm(res1[at1].get_coord() - res2[at2].get_coord())
            distance_map[res1_ind][res2_ind] = dist
            distance_map[res2_ind][res1_ind] = dist
    # Fill the diagonal with 0's
    np.fill_diagonal(distance_map, 0.0)
    return distance_map


# Run the preprocessing here
if not os.path.exists(preprocessed_dir):
    os.mkdir(preprocessed_dir)

for pdb_id_struct in sorted(os.listdir(raw_dir)):
    pre = os.path.join(raw_dir, pdb_id_struct)
    chains = []
    # Check according the fasta files of chains which were generated
    for file in sorted(os.listdir(pre)):
        if file[2:] != "fasta":
            continue
        chain_id = file[0]
        if os.path.exists(
            os.path.join(preprocessed_dir, pdb_id_struct, chain_id + ".npz")
        ):
            continue
        chains.append(chain_id)

    # Means preprocessing of all chains has been done
    if chains == []:
        continue

    # Initialize information required for the complex
    protein = initialize_protein_info(pdb_id_struct, "reindexed_protein.pdb")
    site = initialize_protein_info(pdb_id_struct, "reindexed_site.pdb")
    ligand = initialize_ligand_info(pdb_id_struct)

    for chain_id in chains:
        print(pdb_id_struct, chain_id)
        # Make the dictionary for storage
        try:
            data = {}
            data["pdb_id_struct"] = pdb_id_struct
            data["chain_id"] = chain_id
            data["sequence"] = protein[chain_id]["sequence"]
            data["length"] = len(data["sequence"])
            # Generally should be the case but to avoid corner cases
            if chain_id in site:
                data["labels"] = find_site_residues(protein[chain_id], site[chain_id])
            else:
                print("Following not part of binding site", pdb_id_struct, chain_id)
                data["labels"] = np.zeros(len(protein[chain_id]["sequence"]))
            # For structure-based prediction
            data["dist_map_true"] = get_distance_map_true(protein[chain_id])
            # For penalising the loss function better
            data["prot_lig_dist"] = get_protein_ligand_dist(protein[chain_id], ligand)
        except:  # noqa: E722
            print(pdb_id_struct, chain_id, "dictionary fail")
            continue

        # Write the data to a numpy .npz file
        folder = os.path.join(preprocessed_dir, pdb_id_struct)
        if not os.path.exists(folder):
            os.mkdir(folder)
        np.savez(os.path.join(folder, chain_id + ".npz"), **data)


# %%
# Let us take all the amino acid properties available from the AAindex database
# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238890/
# The file AA_properties in the data folder contains all the values
# We will take only the features which are the least correlated as our features
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib import cm as cm

tmp_df = pd.read_csv("../data/AA_properties.csv", sep=",")
df = tmp_df.iloc[:, 7:].T


# Function used to normalize the values between 0 and 1
def normalize(df):
    result = df.copy()
    for feature_name in df.columns:
        max_value = df[feature_name].max()
        min_value = df[feature_name].min()
        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)
    return result


# A way to view the correlation matrix
def correlation_matrix(df):
    fig = plt.figure()
    ax1 = fig.add_subplot(111)
    cmap = cm.get_cmap("jet", 30)
    cax = ax1.imshow(df.corr(), interpolation="nearest", cmap=cmap)
    ax1.grid(True)
    plt.title("Feature Correlation")
    labels = np.arange(0, len(df), 1)
    ax1.set_xticklabels(labels, fontsize=6)
    ax1.set_yticklabels(labels, fontsize=6)
    # Add colorbar, make sure to specify tick locations to match desired ticklabels
    fig.colorbar(cax, ticks=np.arange(-1.1, 1.1, 0.1))
    plt.show()


# Using spearman correlation
corr = df.corr("spearman")
threshold = 0.6
columns = np.full((corr.shape[0],), True, dtype=bool)
for i in range(corr.shape[0]):
    for j in range(i + 1, corr.shape[0]):
        if corr.iloc[i, j] >= threshold or corr.iloc[i, j] <= -threshold:
            if columns[j]:
                columns[j] = False
selected_columns = df.columns[columns]
features = df[selected_columns]

# Saving all the features and the selected features
normalize(df).to_csv("../data/all_features.csv")
normalize(features).to_csv("../data/selected_features.csv")


# %%
# Assuming that all the PSSMs have been copied to data/scPDB/pssm
# We can have more features included as well. For now, let us consider PSSMs
import csv
from collections import defaultdict

import numpy as np


# Get amino acid properties from the created files above
def get_amino_acid_properties(csv_file):
    feats = {}
    with open(csv_file) as f:
        records = csv.reader(f)
        for i, row in enumerate(records):
            if i == 0:
                length = len(row) - 1
                continue
            feats[row[0]] = np.array(
                [float(el) if el != "" else 0.0 for el in row[1:]], dtype=np.float32
            )
        feats["X"] = np.zeros(length)
    feats = defaultdict(lambda: np.zeros(length), feats)
    return feats


# List of amino acids and their integer representation
AA_ID_DICT = {
    "X": 0,
    "A": 1,
    "C": 2,
    "D": 3,
    "E": 4,
    "F": 5,
    "G": 6,
    "H": 7,
    "I": 8,
    "K": 9,
    "L": 10,
    "M": 11,
    "N": 12,
    "P": 13,
    "Q": 14,
    "R": 15,
    "S": 16,
    "T": 17,
    "V": 18,
    "W": 19,
    "Y": 20,
}
AA_ID_DICT = defaultdict(lambda: 0, AA_ID_DICT)

# We generated PSSMs for select sequences
# Mapping different sequences to the one for which we generated
common_pssms = defaultdict(str)
with open(os.path.join(data_dir, "unique"), "r") as f:
    for line in f.readlines():
        line = line.strip().split()
        pssm_generated_for = line[0][:-1]
        common_pssms[pssm_generated_for] = pssm_generated_for
        for pdb_id_struct_chain in line[1:]:
            pdb_id_struct_chain = pdb_id_struct_chain[:-1]
            common_pssms[pdb_id_struct_chain] = pssm_generated_for


def get_pssm(pdb_id_struct, chain_id, length):
    pssm_pdb_id_struct, pssm_chain_id = common_pssms[
        pdb_id_struct + "/" + chain_id
    ].split("/")
    with open(
        os.path.join(pssm_dir, pssm_pdb_id_struct, pssm_chain_id + ".pssm"), "r"
    ) as f:
        lines = f.readlines()
    feature = np.zeros((21, length))
    for i, line in enumerate(lines):
        feature[i] = np.array(line.strip().split(), dtype=np.float32)
    return feature


# Amino acid physico-chemical features selected by removing highly correlated features
AA_sel_feats = get_amino_acid_properties(
    os.path.join(os.path.dirname(data_dir), "selected_features.csv")
)


# %%
# Now, let us preprocess the files again to generate the features directly that can be imported into pytorch easily
# For that we can define the generate_input function which can be used to generate various types of inputs
import numpy as np


# Without using distance map
feat_vec_len = 21 + 1 + 21 + len(AA_sel_feats["X"])


def generate_input(sample):
    X = np.zeros((feat_vec_len, sample["length"]))

    # One-hot encoding
    X[:21] = np.array([np.eye(21)[AA_ID_DICT[el]] for el in sample["sequence"]]).T

    # Positional encoding
    X[21] = np.arange(1, sample["length"] + 1, dtype=np.float32) / sample["length"]

    # PSSM
    X[22:43] = sample["pssm"]

    # AA Properties
    X[43:] = np.array([AA_sel_feats[aa] for aa in sample["sequence"]]).T

    return X


# Using the distance map to pick the closest 20 residues and creating features for each amino acid
# Amino acid no. + Distance from curr. aa + PSSM + AA Properties

feat_vec_len = 21 + 1 + 21 + len(AA_sel_feats["X"])
close_aa = 20


# def generate_input(sample):
#     X = np.zeros((feat_vec_len * close_aa, sample["length"]))
#     for i, aa_dist in enumerate(sample["ca_dist_map_true"]):
#         if sample["length"] <= close_aa:
#             indices = np.argsort(aa_dist)
#         else:
#             # Selects the smallest `close_aa` elements not in sorted order
#             indices = list(np.argpartition(aa_dist, close_aa)[:close_aa])
#             # Sort the indices according to their value
#             indices.sort(key=lambda x: aa_dist[x])
#         for j, idx in enumerate(indices):
#             if aa_dist[idx] == 1e6:
#                 # Implies we don't have structural info about any AA after this
#                 break
#             aa = sample["sequence"][idx]
#             X[j * feat_vec_len : j * feat_vec_len + 21, i] = np.eye(21)[AA_ID_DICT[aa]]
#             X[j * feat_vec_len + 21, i] = aa_dist[idx]
#             X[j * feat_vec_len + 22 : j * feat_vec_len + 43, i] = sample["pssm"][:, idx]
#             X[j * feat_vec_len + 43 : (j + 1) * feat_vec_len, i] = np.array(
#                 AA_sel_feats[aa]
#             )
#     return X


# With an inverted distance map
# This does not work very well

# def generate_input(sample):
#     X = np.zeros((feat_vec_len, sample["length"]))

#     # One-hot encoding
#     X[:21] = np.array([np.eye(21)[AA_ID_DICT[el]] for el in sample["sequence"]]).T

#     # PSSM
#     X[22:43] = sample["pssm"]

#     # AA Properties
#     X[43:] = np.array([AA_sel_feats[aa] for aa in sample["sequence"]]).T

#     # Invert the distance map and matrix multiply with X so that we get a combination of all features
#     inverted_dist = 1 / sample["ca_dist_map_true"]
#     np.fill_diagonal(inverted_dist, 1.0)
#     X_T = inverted_dist.dot(X.T)
#     for i in range(sample["length"]):
#         X_T[i] = X_T[i] / np.sum(inverted_dist[i])

#     X = X_T.T

#     # Positional encoding
#     X[21] = np.arange(1, sample["length"] + 1, dtype=np.float32) / sample["length"]

#     return X


# %%
# USING CONCATENATION STRATEGY

for pdb_id_struct in sorted(os.listdir(preprocessed_dir)):
    flg = True
    pre = os.path.join(preprocessed_dir, pdb_id_struct)
    features_file = os.path.join(pre, "features.npy")
    labels_file = os.path.join(pre, "labels.npy")

    if os.path.exists(labels_file):
        continue
    print(pdb_id_struct)

    for file in sorted(os.listdir(pre)):
        # In case features were generated but not labels, redo it
        if file == "features.npy":
            continue
        chain_id = file[-len(".npz") - 1 : -len(".npz")]
        sample = np.load(os.path.join(pre, file))
        sample = {
            key: sample[key].item() if sample[key].shape == () else sample[key]
            for key in sample
        }
        sample["pssm"] = get_pssm(pdb_id_struct, chain_id, sample["length"])
        if flg:
            X = generate_input(sample)
            y = sample["labels"]
            flg = False
        else:
            # Using concatenation strategy
            tmp = generate_input(sample)
            X = np.concatenate((X, tmp), 1)
            y = np.concatenate((y, sample["labels"]), 0)

    np.save(features_file, X)
    np.save(labels_file, y)


# SAVING ALL CHAINS AS DIFFERENT PROTEINS
# for pdb_id_struct in sorted(os.listdir(preprocessed_dir)):
#     pre = os.path.join(preprocessed_dir, pdb_id_struct)
#     print(pdb_id_struct)

#     for file in sorted(os.listdir(pre)):
#         # In case features were generated but not labels, redo it
#         if not file.endswith(".npz"):
#             continue
#         chain_id = file[-len(".npz") - 1 : -len(".npz")]
#         features_file = os.path.join(pre, "features" + chain_id + ".npy")
#         labels_file = os.path.join(pre, "labels" + chain_id + ".npy")
#         if os.path.exists(features_file) and os.path.exists(labels_file):
#             continue
#         sample = np.load(os.path.join(pre, file))
#         sample = {
#             key: sample[key].item() if sample[key].shape is () else sample[key]
#             for key in sample
#         }
#         sample["pssm"] = get_pssm(pdb_id_struct, chain_id, sample["length"])
#         X = generate_input(sample)
#         y = sample["labels"]

#         np.save(features_file, X)
#         np.save(labels_file, y)


# %%
# Let us save the above files and store in a safe space


def archive_dir(folder, pattern, name):
    parent_dir = os.path.dirname(folder)
    folder = os.path.basename(folder)
    #     print(parent_dir, folder)
    if os.path.exists(os.path.join(parent_dir, name)):
        print("Warning:", name, "already exists in", parent_dir)
        inp = input("Do you want to overwrite existing file? (y/n): ")
        if inp[0].lower() == "n":
            return
    # Using only a single ! command since multiple ! spawn different bash shells
    # For some reason, the below code is doubling the contents of the tar file
    # !cd $parent_dir; find $folder -name "$pattern" | tar --sort=name -I zstd -cf $name -T -;
    # rsync -avP $name crvineeth97@ada:/share2/crvineeth97/compressed/scPDB; cd -
    # To untar, use
    # !tar -I zstd -xf $name


# archive_dir(raw_dir, "*", "raw.tar.zst")
# archive_dir(preprocessed_dir, "*.npy", "features_labels.tar.zst")
# archive_dir(preprocessed_dir, "*.npz", "preprocessed_chains.tar.zst")
# archive_dir(pssm_dir, "*", "pssm.tar.zst")
