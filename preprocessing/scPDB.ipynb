{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitskorchcondae55dff3dc5444e92a7d51787ddcd9901",
   "display_name": "Python 3.7.6 64-bit ('skorch': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the scPDB dataset and extract the dataset in \"data/scPDB/raw\"\n",
    "!aria2c -c -x 8 -s 8 -d \"../data/scPDB\" http://bioinfo-pharma.u-strasbg.fr/scPDB/ressources/2016/scPDB.tar.gz --out 'scPDB.tar.gz'\n",
    "!tar xvzf ../data/scPDB/scPDB.tar.gz -C ../data/scPDB/raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 10-fold Cross Validation, we will use the splits that were generated by https://arxiv.org/abs/1904.06517\n",
    "!aria2c -c -x 8 -s 8 -d \"../data/scPDB\" https://gitlab.com/cheminfIBB/kalasanty/-/archive/master/kalasanty-master.tar.gz?path=data --out 'kalasanty-master-data.tar.gz'\n",
    "!tar xvzf ../data/scPDB/kalasanty-master-data.tar.gz -C ../data/scPDB/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants that will be required\n",
    "# ALWAYS RUN THIS CODE CELL\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "data_dir = os.path.abspath(\"../data/scPDB\")\n",
    "raw_dir = os.path.join(data_dir, \"raw\")\n",
    "pssm_dir = os.path.join(data_dir, \"pssm\")\n",
    "splits_dir = os.path.join(data_dir, \"splits\")\n",
    "preprocessed_dir = os.path.join(data_dir, \"preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sequence-based prediction, we need to use RCSB FASTA files and since scPDB has only mol2 files for the proteins, we will download the fasta file and PDB files of the given proteins ourself. This allows for proper calculation of the labels\n",
    "# Note that the downloaded PDB automatically has all the structures of a particular PDB ID\n",
    "# Hence, we just use the first structure instead of all\n",
    "# Download sequence and PDB files from RCSB for easier matching of labels\n",
    "import urllib\n",
    "\n",
    "for file in sorted(os.listdir(raw_dir)):\n",
    "    pdb_id = file[:4]\n",
    "    print(pdb_id)\n",
    "\n",
    "    pdb_save = path.join(folder, file, \"downloaded.pdb\")\n",
    "    if not path.exists(pdb_save):\n",
    "        try:\n",
    "            urllib.request.urlretrieve('http://files.rcsb.org/download/' + pdb_id + \".pdb\", pdb_save)\n",
    "        except:\n",
    "            print(\"Err: pdb \" + pdb_id)\n",
    "\n",
    "    fasta_save = path.join(folder, file, \"sequence.fasta\")\n",
    "    if not path.exists(fasta_save):\n",
    "        try:\n",
    "            urllib.request.urlretrieve('https://www.rcsb.org/pdb/download/downloadFastaFiles.do?structureIdList=' + pdb_id + '&compressionType=uncompressed', fasta_save)\n",
    "        except:\n",
    "            print(\"Err: fasta \" + pdb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether downloaded PDB and sequence files are correct\n",
    "# A few of the PDB files have been obseleted and hence will have to manually download them\n",
    "import subprocess\n",
    "\n",
    "def last_line(f):\n",
    "    proc = subprocess.Popen(['tail', '-n', \"1\", f], stdout=subprocess.PIPE)\n",
    "    line = proc.stdout.readlines()[0].decode(\"utf-8\").strip()\n",
    "    return line\n",
    "\n",
    "for file in sorted(os.listdir(raw_dir)):\n",
    "    pdb_id = file[:4]\n",
    "    pdb_save = os.path.join(raw_dir, file, \"downloaded.pdb\")\n",
    "    if last_line(pdb_save) != \"END\":\n",
    "        print(\"Err: PDB \" + file)\n",
    "    fasta_save = os.path.join(raw_dir, file, \"sequence.fasta\")\n",
    "    with open(fasta_save, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        if line[0] != \">\":\n",
    "            print(\"Err: FASTA \" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to generate MSAs for the protein sequences in the dataset\n",
    "# For that, we need to split the sequence.fasta file into respective chain.fasta files\n",
    "# Also, we need to remove the fasta files of DNA/RNA seqeuences\n",
    "\n",
    "for file in sorted(os.listdir(raw_dir)):\n",
    "    file = file.strip()\n",
    "    pre = os.path.join(raw_dir, file)\n",
    "    \n",
    "    # Read SEQRES entries in PDB file to determine whether a chain\n",
    "    # has a protein sequence or not\n",
    "    pdb_file = os.path.join(pre, \"downloaded.pdb\")\n",
    "    do_not_include = set()\n",
    "    with open(pdb_file, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line[:6] != \"SEQRES\":\n",
    "            line = f.readline()\n",
    "        while line[:6] == \"SEQRES\":\n",
    "            chain_id = line[11]\n",
    "            residue = line[19:22]\n",
    "            # Generally DNA/RNA have 1 or 2-letter codes\n",
    "            if \" \" in residue:\n",
    "                do_not_include.add(chain_id)\n",
    "            line = f.readline()\n",
    "    \n",
    "    fasta = os.path.join(pre, \"sequence.fasta\")\n",
    "    with open(fasta, \"r\") as f:\n",
    "        header = f.readline()\n",
    "        while 1:\n",
    "            chain_id = header[6:7]\n",
    "            sequence = \"\"\n",
    "            line = f.readline()\n",
    "            while line != \"\" and line is not None and line[0] != \">\":\n",
    "                sequence += line.strip()\n",
    "                line = f.readline()\n",
    "            if chain_id not in do_not_include:\n",
    "                with open(os.path.join(pre, chain_id + \".fasta\"), \"w\") as hlp:\n",
    "                    hlp.write(header)\n",
    "                    hlp.write(sequence + \"\\n\")\n",
    "            if line == \"\" or line is None:\n",
    "                break\n",
    "            header = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you want to delete the generated fasta files from the above cell, use this\n",
    "# for file in sorted(os.listdir(raw_dir)):\n",
    "#     for fasta in glob(os.path.join(raw_dir, file.strip(), \"?.fasta\")):\n",
    "#         os.remove(fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fasta files generated will have a lot of common sequences\n",
    "# To speed up MSA generation, let us create a unique file that has common sequences\n",
    "# Then we can generate the MSAs for only the first chain in every line\n",
    "from collections import defaultdict\n",
    "\n",
    "sequences = defaultdict(list)\n",
    "for file in sorted(os.listdir(raw_dir)):\n",
    "    pre = os.path.join(raw_dir, file.strip())\n",
    "    for fasta in sorted(os.listdir(pre)):\n",
    "        if fasta[2:] != \"fasta\":\n",
    "            continue\n",
    "        chain_id = fasta[0]\n",
    "        with open(os.path.join(pre, fasta)) as f:\n",
    "            f.readline()\n",
    "            sequence = f.readline().strip()\n",
    "            # This choice was made so that rsync would work much better and easier\n",
    "            sequences[sequence].append(file + \"/\" + chain_id + \"*\")\n",
    "\n",
    "keys = list(sequences.keys())\n",
    "\n",
    "with open(os.path.join(data_dir, \"unique\"), \"w\") as f:\n",
    "    for key in keys:\n",
    "        line = \"\"\n",
    "        for chain_id in sequences[key]:\n",
    "            line += chain_id + \" \"\n",
    "        f.write(line[:-1] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a regex search on the generated fasta files to ensure that we don't have any DNA/RNA sequences\n",
    "# Will have to manually check the files to ensure that they are protein sequences\n",
    "# All of them are protein sequences\n",
    "\n",
    "import re\n",
    "\n",
    "def match(strg, search=re.compile(r\"[^ACGTURYKMSWBDHVN\\-\\.]\").search):\n",
    "    return not bool(search(strg))\n",
    "\n",
    "with open(os.path.join(data_dir, \"unique\"), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    pdb_id_struct, chain_id = line.strip().split()[0].split(\"/\")\n",
    "    chain_id = chain_id[0]\n",
    "    with open(os.path.join(raw_dir, pdb_id_struct, chain_id + \".fasta\"), \"r\") as f:\n",
    "        f.readline()\n",
    "        seq = f.readline().strip()\n",
    "    if match(seq):\n",
    "        print(pdb_id_struct, chain_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSAs need to be generated for the fasta files\n",
    "# Refer to https://github.com/crvineeth97/msa-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKING OF .npz FILES FROM HERE\n",
    "# Let us preprocess ALL the available data and create .npz files which contain pdb_id, chain_id, sequence, length, labels\n",
    "import numpy as np\n",
    "\n",
    "# Write the code for this here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the selected features of amino acids\n",
    "# Code here\n",
    "\n",
    "def get_amino_acid_features(csv_file):\n",
    "    feats = {}\n",
    "    with open(csv_file) as f:\n",
    "        records = csv.reader(f)\n",
    "        for i, row in enumerate(records):\n",
    "            if i == 0:\n",
    "                length = len(row) - 1\n",
    "                continue\n",
    "            feats[row[0]] = np.array(\n",
    "                [float(el) if el != \"\" else 0.0 for el in row[1:]], dtype=np.float32\n",
    "            )\n",
    "        feats[\"X\"] = np.zeros(length)\n",
    "    feats = defaultdict(lambda: np.zeros(length), feats)\n",
    "    return feats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that all the PSSMs have been copied to data/scPDB/pssm\n",
    "# We can have more features included as well. For now, let us consider PSSMs\n",
    "# !rsync -avP --include=\"*/\" --include=\"*pssm\" --exclude=\"*\" ~/Git/msa-generator/data/scPDB/ ~/Git/protein-binding-site-prediction/data/scPDB/pssm/\n",
    "# Now, let us preprocess the files again to generate the features directly that can be imported into pytorch easily\n",
    "\n",
    "# USING CONCATENATION STRATEGY\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# List of amino acids and their integer representation\n",
    "AA_ID_DICT = {\n",
    "    \"X\": 0,\n",
    "    \"A\": 1,\n",
    "    \"C\": 2,\n",
    "    \"D\": 3,\n",
    "    \"E\": 4,\n",
    "    \"F\": 5,\n",
    "    \"G\": 6,\n",
    "    \"H\": 7,\n",
    "    \"I\": 8,\n",
    "    \"K\": 9,\n",
    "    \"L\": 10,\n",
    "    \"M\": 11,\n",
    "    \"N\": 12,\n",
    "    \"P\": 13,\n",
    "    \"Q\": 14,\n",
    "    \"R\": 15,\n",
    "    \"S\": 16,\n",
    "    \"T\": 17,\n",
    "    \"V\": 18,\n",
    "    \"W\": 19,\n",
    "    \"Y\": 20,\n",
    "}\n",
    "AA_ID_DICT = defaultdict(lambda: 0, AA_ID_DICT)\n",
    "\n",
    "# One-hot encoding and positional encoding\n",
    "feat_vec_len = 21\n",
    "feat_vec_len += 1\n",
    "\n",
    "# We generated PSSMs for select sequences\n",
    "# Mapping different sequences to the one for which we generated\n",
    "common_pssms = defaultdict(str)\n",
    "with open(os.path.join(data_dir, \"unique\"), \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split()\n",
    "        pssm_generated_for = line[0][:-1]\n",
    "        common_pssms[pssm_generated_for] = pssm_generated_for\n",
    "        for pdb_id_struct_chain in line[1:]:\n",
    "            pdb_id_struct_chain = pdb_id_struct_chain[:-1]\n",
    "            common_pssms[pdb_id_struct_chain] = pssm_generated_for\n",
    "\n",
    "\n",
    "def get_pssm(pdb_id_struct, chain_id, length):\n",
    "    pssm_pdb_id_struct, pssm_chain_id = common_pssms[pdb_id_struct + \"/\" + chain_id].split(\"/\")\n",
    "    with open(os.path.join(pssm_dir, pssm_pdb_id_struct, pssm_chain_id + \".pssm\"), \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    feature = np.zeros((21, length))\n",
    "    for i, line in enumerate(lines):\n",
    "        feature[i] = np.array(line.strip().split(), dtype=np.float32)\n",
    "    return feature\n",
    "\n",
    "# PSSM length\n",
    "feat_vec_len += 21\n",
    "\n",
    "# Amino acid physico-chemical features selected by removing highly correlated features\n",
    "AA_sel_feats = get_amino_acid_features(os.path.join(data_dir, \"selected_features.csv\"))\n",
    "feat_vec_len += len(AA_sel_feats[\"X\"])\n",
    "\n",
    "\n",
    "def generate_input(sample):\n",
    "    \"\"\"\n",
    "    Generate input for a single sample which is a dictionary containing required items\n",
    "    \"\"\"\n",
    "    X = np.zeros((feat_vec_len, sample[\"length\"]))\n",
    "\n",
    "    # One-hot encoding\n",
    "    X[:21] = np.array([np.eye(21)[AA_ID_DICT[el]] for el in sample[\"sequence\"]]).T\n",
    "\n",
    "    # Positional encoding\n",
    "    X[21] = np.arange(1, sample[\"length\"] + 1, dtype=np.float32) / sample[\"length\"]\n",
    "\n",
    "    # PSSM\n",
    "    X[22:43] = sample[\"pssm\"]\n",
    "\n",
    "    # AA Properties\n",
    "    X[43:] = np.array([AA_sel_feats[aa] for aa in sample[\"sequence\"]]).T\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "for pdb_id_struct in sorted(os.listdir(preprocessed_dir)):\n",
    "    flg = True\n",
    "    pre = os.path.join(preprocessed_dir, pdb_id_struct)\n",
    "    features_file = os.path.join(pre, \"features.npy\")\n",
    "    labels_file = os.path.join(pre, \"labels.npy\")\n",
    "\n",
    "    if os.path.exists(labels_file):\n",
    "        continue\n",
    "    print(pdb_id_struct)\n",
    "\n",
    "    for file in sorted(os.listdir(pre)):\n",
    "        # In case features were generated but not labels, redo it\n",
    "        if file == \"features.npz\":\n",
    "            continue\n",
    "        chain_id = file[-len(\".npz\") - 1 : -len(\".npz\")]\n",
    "        sample = np.load(os.path.join(pre, file))\n",
    "        sample = {\n",
    "            key: sample[key].item() if sample[key].shape is () else sample[key]\n",
    "            for key in sample\n",
    "        }\n",
    "        sample[\"pssm\"] = get_pssm(pdb_id_struct, chain_id, sample[\"length\"])\n",
    "        if flg:\n",
    "            X = generate_input(sample)\n",
    "            y = sample[\"labels\"]\n",
    "            flg = False\n",
    "        else:\n",
    "            # Using concatenation strategy\n",
    "            tmp = generate_input(sample)\n",
    "            X = np.concatenate((X, tmp), 1)\n",
    "            y = np.concatenate((y, sample[\"labels\"]), 0)\n",
    "\n",
    "    np.save(features_file, X)\n",
    "    np.save(labels_file, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISSUES\n",
    "\n",
    "<input type=\"checkbox\"> Improve downloading PDB and fasta by using something that resumes downloads\n",
    "\n",
    "<input type=\"checkbox\"> Not sure if checking for a space in the residue is the best way of checking. Can use the code_with_modified_residues dictionary from NWalign.py (https://zhanglab.ccmb.med.umich.edu/NW-align/NWalign.py)\n",
    "\n",
    "<input type=\"checkbox\"> Make the pdb_id field in the preprocessed files into pdb_id_struct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}