{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants that will be required\n",
    "# ALWAYS RUN THIS CODE CELL\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "data_dir = os.path.abspath(\"../data/scPDB\")\n",
    "raw_dir = os.path.join(data_dir, \"raw\")\n",
    "pssm_dir = os.path.join(data_dir, \"pssm\")\n",
    "splits_dir = os.path.join(data_dir, \"splits\")\n",
    "preprocessed_dir = os.path.join(data_dir, \"preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the scPDB dataset and extract the dataset in \"data/scPDB/raw\"\n",
    "!aria2c -c -x 8 -s 8 -d \"../data/scPDB\" http://bioinfo-pharma.u-strasbg.fr/scPDB/ressources/2016/scPDB.tar.gz --out 'scPDB.tar.gz'\n",
    "!tar xvzf ../data/scPDB/scPDB.tar.gz -C ../data/scPDB/raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 10-fold Cross Validation, we will use the splits that were generated by https://arxiv.org/abs/1904.06517\n",
    "!aria2c -c -x 8 -s 8 -d \"../data/scPDB\" https://gitlab.com/cheminfIBB/kalasanty/-/archive/master/kalasanty-master.tar.gz?path=data --out 'kalasanty-master-data.tar.gz'\n",
    "!tar xvzf ../data/scPDB/kalasanty-master-data.tar.gz -C ../data/scPDB/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sequence-based prediction, we need to use RCSB FASTA files and since scPDB has only mol2 files for the proteins, we will download the fasta file and PDB files of the given proteins ourself. This allows for proper calculation of the labels\n",
    "# Note that the downloaded PDB automatically has all the structures of a particular PDB ID\n",
    "# Hence, we just use the first structure instead of all\n",
    "# Download sequence and PDB files from RCSB for easier matching of labels\n",
    "import urllib\n",
    "\n",
    "for file in sorted(os.listdir(raw_dir)):\n",
    "    pdb_id = file[:4]\n",
    "    print(pdb_id)\n",
    "\n",
    "    pdb_save = path.join(folder, file, \"downloaded.pdb\")\n",
    "    if not path.exists(pdb_save):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(\n",
    "                \"http://files.rcsb.org/download/\" + pdb_id + \".pdb\", pdb_save\n",
    "            )\n",
    "        except:\n",
    "            print(\"Err: pdb \" + pdb_id)\n",
    "\n",
    "    fasta_save = path.join(folder, file, \"sequence.fasta\")\n",
    "    if not path.exists(fasta_save):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://www.rcsb.org/pdb/download/downloadFastaFiles.do?structureIdList=\"\n",
    "                + pdb_id\n",
    "                + \"&compressionType=uncompressed\",\n",
    "                fasta_save,\n",
    "            )\n",
    "        except:\n",
    "            print(\"Err: fasta \" + pdb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether downloaded PDB and sequence files are correct\n",
    "# A few of the PDB files have been obseleted and hence will have to manually download them\n",
    "# I wrote down the manual mapping of the pdb id's but seem to have lost it\n",
    "# But they can easily be found out from data/obseleted.txt file\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def last_line(f):\n",
    "    proc = subprocess.Popen([\"tail\", \"-n\", \"1\", f], stdout=subprocess.PIPE)\n",
    "    line = proc.stdout.readlines()[0].decode(\"utf-8\").strip()\n",
    "    return line\n",
    "\n",
    "\n",
    "for file in sorted(os.listdir(raw_dir)):\n",
    "    pdb_id = file[:4]\n",
    "    pdb_save = os.path.join(raw_dir, file, \"downloaded.pdb\")\n",
    "    if last_line(pdb_save) != \"END\":\n",
    "        print(\"Err: PDB \" + file)\n",
    "    fasta_save = os.path.join(raw_dir, file, \"sequence.fasta\")\n",
    "    with open(fasta_save, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        if line[0] != \">\":\n",
    "            print(\"Err: FASTA \" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to generate MSAs for the protein sequences in the dataset\n",
    "# For that, we need to split the sequence.fasta file into respective chain.fasta files\n",
    "# Also, we need to remove the fasta files of DNA/RNA seqeuences\n",
    "\n",
    "for file in sorted(os.listdir(raw_dir)):\n",
    "    file = file.strip()\n",
    "    pre = os.path.join(raw_dir, file)\n",
    "\n",
    "    # Read SEQRES entries in PDB file to determine whether a chain\n",
    "    # has a protein sequence or not\n",
    "    pdb_file = os.path.join(pre, \"downloaded.pdb\")\n",
    "    do_not_include = set()\n",
    "    with open(pdb_file, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line[:6] != \"SEQRES\":\n",
    "            line = f.readline()\n",
    "        while line[:6] == \"SEQRES\":\n",
    "            chain_id = line[11]\n",
    "            residue = line[19:22]\n",
    "            # Generally DNA/RNA have 1 or 2-letter codes\n",
    "            if \" \" in residue:\n",
    "                do_not_include.add(chain_id)\n",
    "            line = f.readline()\n",
    "\n",
    "    fasta = os.path.join(pre, \"sequence.fasta\")\n",
    "    with open(fasta, \"r\") as f:\n",
    "        header = f.readline()\n",
    "        while 1:\n",
    "            chain_id = header[6:7]\n",
    "            sequence = \"\"\n",
    "            line = f.readline()\n",
    "            while line != \"\" and line is not None and line[0] != \">\":\n",
    "                sequence += line.strip()\n",
    "                line = f.readline()\n",
    "            if chain_id not in do_not_include:\n",
    "                with open(os.path.join(pre, chain_id + \".fasta\"), \"w\") as hlp:\n",
    "                    hlp.write(header)\n",
    "                    hlp.write(sequence + \"\\n\")\n",
    "            if line == \"\" or line is None:\n",
    "                break\n",
    "            header = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you want to delete the generated fasta files from the above cell, use this\n",
    "# for file in sorted(os.listdir(raw_dir)):\n",
    "#     for fasta in glob(os.path.join(raw_dir, file.strip(), \"?.fasta\")):\n",
    "#         os.remove(fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us remove some other troublesome fasta files\n",
    "# trouble = [\"1m1d_1/D.fasta\", # The PDB file does not contain structure of this sequence at all\n",
    "#             \"2xbm_4/E.fasta\", # RNA sequence that slipped past somehow\n",
    "#             \"2xbm_4/F.fasta\"] # RNA sequence that slipped past somehow\n",
    "# for chain in trouble:\n",
    "#     file = os.path.join(raw_dir, chain)\n",
    "#     if os.path.exists(file):\n",
    "#         os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fasta files generated will have a lot of common sequences\n",
    "# To speed up MSA generation, let us create a unique file that has common sequences\n",
    "# Then we can generate the MSAs for only the first chain in every line\n",
    "from collections import defaultdict\n",
    "\n",
    "sequences = defaultdict(list)\n",
    "for file in sorted(os.listdir(raw_dir)):\n",
    "    pre = os.path.join(raw_dir, file.strip())\n",
    "    for fasta in sorted(os.listdir(pre)):\n",
    "        if fasta[2:] != \"fasta\":\n",
    "            continue\n",
    "        chain_id = fasta[0]\n",
    "        with open(os.path.join(pre, fasta)) as f:\n",
    "            f.readline()\n",
    "            sequence = f.readline().strip()\n",
    "            # This choice was made so that rsync would work much better and easier\n",
    "            sequences[sequence].append(file + \"/\" + chain_id + \"*\")\n",
    "\n",
    "keys = list(sequences.keys())\n",
    "\n",
    "with open(os.path.join(data_dir, \"unique\"), \"w\") as f:\n",
    "    for key in keys:\n",
    "        line = \"\"\n",
    "        for chain_id in sequences[key]:\n",
    "            line += chain_id + \" \"\n",
    "        f.write(line[:-1] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a regex search on the generated fasta files to ensure that we don't have any DNA/RNA sequences\n",
    "# Will have to manually check the files to ensure that they are protein sequences\n",
    "# All of them are protein sequences\n",
    "import re\n",
    "\n",
    "\n",
    "def match(strg, search=re.compile(r\"[^ACGTURYKMSWBDHVN\\-\\.]\").search):\n",
    "    return not bool(search(strg))\n",
    "\n",
    "\n",
    "with open(os.path.join(data_dir, \"unique\"), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    pdb_id_struct, chain_id = line.strip().split()[0].split(\"/\")\n",
    "    chain_id = chain_id[0]\n",
    "    with open(os.path.join(raw_dir, pdb_id_struct, chain_id + \".fasta\"), \"r\") as f:\n",
    "        f.readline()\n",
    "        seq = f.readline().strip()\n",
    "    if match(seq):\n",
    "        print(pdb_id_struct, chain_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSAs need to be generated for the fasta files\n",
    "# Refer to https://github.com/crvineeth97/msa-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKING OF .npz FILES FROM HERE\n",
    "# Download NWalign.py, pdb2fasta.py and reindex_pdb.py\n",
    "# https://zhanglab.ccmb.med.umich.edu/NW-align/NWalign.py (Make small changes for Python3 compatibility)\n",
    "# https://zhanglab.ccmb.med.umich.edu/reindex_pdb/reindex_pdb.py (Make changes to allow for reindexing specific chains and also to replace celenocysteine, \"U\" with \"X\")\n",
    "# https://zhanglab.ccmb.med.umich.edu/reindex_pdb/pdb2fasta.py (Almost the same)\n",
    "# Let us preprocess ALL the available data and create .npz files which contain pdb_id, chain_id, sequence, length, labels\n",
    "# %debug\n",
    "# from IPython.core.debugger import set_trace\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from Bio import BiopythonWarning\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "from rdkit import Chem\n",
    "from reindex_pdb import reindex_pdb\n",
    "\n",
    "warnings.simplefilter(\"ignore\", BiopythonWarning)\n",
    "parser = PDBParser()\n",
    "\n",
    "\n",
    "def initialize_protein_info(pdb_id_struct, chain_id):\n",
    "    pre = os.path.join(raw_dir, pdb_id_struct)\n",
    "    protein = {}\n",
    "    protein[\"structure\"] = parser.get_structure(\n",
    "        pdb_id_struct, os.path.join(pre, \"tmp.pdb\")\n",
    "    )\n",
    "\n",
    "    protein[\"residues\"] = []\n",
    "    for res in protein[\"structure\"][0][chain_id]:\n",
    "        id = res.get_id()\n",
    "        if is_aa(res, standard=False) and id[0] == \" \":\n",
    "            protein[\"residues\"].append(res)\n",
    "\n",
    "    protein[\"sequence\"] = \"\"\n",
    "    with open(os.path.join(pre, chain_id + \".fasta\")) as f:\n",
    "        line = f.readline()\n",
    "        line = f.readline()\n",
    "        while line != \"\" and line is not None:\n",
    "            protein[\"sequence\"] += line.strip()\n",
    "            line = f.readline()\n",
    "    return protein\n",
    "\n",
    "\n",
    "def initialize_ligand_info(pdb_id_struct, chain_id):\n",
    "    pre = os.path.join(raw_dir, pdb_id_struct)\n",
    "    ligand = {}\n",
    "    ligand[\"supplier\"] = Chem.SDMolSupplier(\n",
    "        os.path.join(pre, \"ligand.sdf\"), sanitize=False\n",
    "    )\n",
    "    assert len(ligand[\"supplier\"]) == 1\n",
    "    ligand[\"supplier\"] = ligand[\"supplier\"][0]\n",
    "    assert ligand[\"supplier\"].GetNumConformers() == 1\n",
    "    ligand[\"coords\"] = ligand[\"supplier\"].GetConformer().GetPositions()\n",
    "    ligand[\"num_atoms\"] = ligand[\"supplier\"].GetNumAtoms()\n",
    "    assert ligand[\"num_atoms\"] == len(ligand[\"coords\"])\n",
    "    ligand[\"atom_types\"] = np.array(\n",
    "        [atom.GetSymbol() for atom in ligand[\"supplier\"].GetAtoms()]\n",
    "    )\n",
    "    return ligand\n",
    "\n",
    "\n",
    "def find_residues_in_contact(protein, ligand):\n",
    "    \"\"\"\n",
    "    Returns a numpy 1D array where a 1 represents that the amino acid is in\n",
    "    contact with the ligand\n",
    "    \"\"\"\n",
    "    labels = np.zeros(len(protein[\"sequence\"]))\n",
    "    for residue in protein[\"residues\"]:\n",
    "        res_ind = residue.get_id()[1] - 1\n",
    "        for atom in residue.get_atoms():\n",
    "            if atom.get_fullname()[1] == \"H\":\n",
    "                continue\n",
    "            for i in range(ligand[\"num_atoms\"]):\n",
    "                if ligand[\"atom_types\"][i] == \"H\":\n",
    "                    continue\n",
    "                # We are considering the ligand to be in contact with the AA\n",
    "                # if the distance between them is within 5A\n",
    "                if np.linalg.norm(atom.get_coord() - ligand[\"coords\"][i]) <= 5.0:\n",
    "                    labels[res_ind] = 1\n",
    "                    break\n",
    "            # We know that the residue is in contact with ligand\n",
    "            # So go to the next residue\n",
    "            if labels[res_ind]:\n",
    "                break\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_distance_map_true(protein, atom_type):\n",
    "    length = len(protein[\"sequence\"])\n",
    "    num_residues = len(\n",
    "        protein[\"residues\"]\n",
    "    )  # Might be different from length because of missing residues\n",
    "    distance_map = np.full((length, length), np.inf)  # Initialize to infinite distance\n",
    "    for ind1 in range(num_residues):\n",
    "        res1 = protein[\"residues\"][ind1]\n",
    "        if res1.has_id(atom_type) is False:\n",
    "            continue\n",
    "        res1_ind = res1.get_id()[1] - 1\n",
    "        for ind2 in range(ind1 + 1, num_residues):\n",
    "            res2 = protein[\"residues\"][ind2]\n",
    "            if res2.has_id(atom_type) is False:\n",
    "                continue\n",
    "            res2_ind = res2.get_id()[1] - 1\n",
    "            dist = np.linalg.norm(\n",
    "                res1[atom_type].get_coord() - res2[atom_type].get_coord()\n",
    "            )\n",
    "            distance_map[res1_ind][res2_ind] = dist\n",
    "            distance_map[res2_ind][res1_ind] = dist\n",
    "    np.fill_diagonal(distance_map, 0.0)\n",
    "    return distance_map\n",
    "\n",
    "\n",
    "if not os.path.exists(preprocessed_dir):\n",
    "    os.mkdir(preprocessed_dir)\n",
    "\n",
    "process_time = 0\n",
    "write_time = 0\n",
    "for pdb_id_struct in sorted(os.listdir(raw_dir)):\n",
    "    pre = os.path.join(raw_dir, pdb_id_struct)\n",
    "    if not os.path.exists(os.path.join(pre, \"downloaded.pdb\")):\n",
    "        print(\"Downloaded PDB does not exist for %s\" % pdb_id_struct)\n",
    "        continue\n",
    "    process_time_start = time()\n",
    "    for file in os.listdir(pre):\n",
    "        # Get only the chain fasta sequences\n",
    "        if file[2:] != \"fasta\":\n",
    "            continue\n",
    "        chain_id = file[0]\n",
    "\n",
    "        # If our preprocessed file exists, continue\n",
    "        if os.path.exists(\n",
    "            os.path.join(preprocessed_dir, pdb_id_struct, chain_id + \".npz\")\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        print(pdb_id_struct, chain_id)\n",
    "        # Reindex the chain and write to tmp.pdb\n",
    "        dest = os.path.join(pre, \"tmp.pdb\")\n",
    "        PDBtxt_reindex = reindex_pdb(\n",
    "            os.path.join(pre, chain_id + \".fasta\"),\n",
    "            os.path.join(pre, \"downloaded.pdb\"),\n",
    "            True,\n",
    "        )\n",
    "        # set_trace()\n",
    "        if PDBtxt_reindex is None:\n",
    "            print(pdb_id_struct, chain_id, \"reindex fail\")\n",
    "            continue\n",
    "\n",
    "        with open(dest, \"w\") as fp:\n",
    "            fp.write(PDBtxt_reindex)\n",
    "\n",
    "        # Initialize information required for the complex\n",
    "        protein = initialize_protein_info(pdb_id_struct, chain_id)\n",
    "        ligand = initialize_ligand_info(pdb_id_struct, chain_id)\n",
    "\n",
    "        # Make the dictionary for storage\n",
    "        try:\n",
    "            data = {}\n",
    "            data[\"pdb_id_struct\"] = pdb_id_struct\n",
    "            data[\"chain_id\"] = chain_id\n",
    "            data[\"sequence\"] = protein[\"sequence\"]\n",
    "            data[\"length\"] = len(data[\"sequence\"])\n",
    "            data[\"labels\"] = find_residues_in_contact(protein, ligand)\n",
    "            data[\"ca_dist_map_true\"] = get_distance_map_true(protein, \"CA\")\n",
    "            assert len(data[\"sequence\"]) == len(data[\"labels\"])\n",
    "        except:\n",
    "            print(pdb_id_struct, chain_id, \"dictionary fail\")\n",
    "            continue\n",
    "\n",
    "        # Remove the tmp.pdb file\n",
    "        os.remove(dest)\n",
    "        process_time += time() - process_time_start\n",
    "\n",
    "        # Write the data to a numpy .npz file\n",
    "        write_time_start = time()\n",
    "        folder = os.path.join(preprocessed_dir, pdb_id_struct)\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "        np.savez(os.path.join(folder, chain_id + \".npz\"), **data)\n",
    "        write_time += time() - write_time_start\n",
    "\n",
    "print(\"Processing time:\", process_time)\n",
    "print(\"Write time:\", write_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us take all the amino acid properties available from the AAindex database\n",
    "# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238890/\n",
    "# The file AA_properties in the data folder contains all the values\n",
    "# We will take only the features which are the least correlated as our features\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import cm as cm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "tmp_df = pd.read_csv(\"../data/AA_properties.csv\", sep=\",\")\n",
    "df = tmp_df.iloc[:, 7:].T\n",
    "\n",
    "# Function used to normalize the values between 0 and 1\n",
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "\n",
    "# A way to view the correlation matrix\n",
    "def correlation_matrix(df):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    cmap = cm.get_cmap(\"jet\", 30)\n",
    "    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n",
    "    ax1.grid(True)\n",
    "    plt.title(\"Feature Correlation\")\n",
    "    labels = np.arange(0, len(df), 1)\n",
    "    ax1.set_xticklabels(labels, fontsize=6)\n",
    "    ax1.set_yticklabels(labels, fontsize=6)\n",
    "    # Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
    "    fig.colorbar(cax, ticks=np.arange(-1.1, 1.1, 0.1))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Using spearman correlation\n",
    "corr = df.corr(\"spearman\")\n",
    "threshold = 0.6\n",
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i + 1, corr.shape[0]):\n",
    "        if corr.iloc[i, j] >= threshold or corr.iloc[i, j] <= -threshold:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "selected_columns = df.columns[columns]\n",
    "features = df[selected_columns]\n",
    "\n",
    "# Saving all the features and the selected features\n",
    "normalize(df).to_csv(\"../data/all_features.csv\")\n",
    "normalize(features).to_csv(\"../data/selected_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get amino acid properties from the created files above\n",
    "\n",
    "\n",
    "def get_amino_acid_properties(csv_file):\n",
    "    feats = {}\n",
    "    with open(csv_file) as f:\n",
    "        records = csv.reader(f)\n",
    "        for i, row in enumerate(records):\n",
    "            if i == 0:\n",
    "                length = len(row) - 1\n",
    "                continue\n",
    "            feats[row[0]] = np.array(\n",
    "                [float(el) if el != \"\" else 0.0 for el in row[1:]], dtype=np.float32\n",
    "            )\n",
    "        feats[\"X\"] = np.zeros(length)\n",
    "    feats = defaultdict(lambda: np.zeros(length), feats)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that all the PSSMs have been copied to data/scPDB/pssm\n",
    "# We can have more features included as well. For now, let us consider PSSMs\n",
    "# !rsync -avP --include=\"*/\" --include=\"*pssm\" --exclude=\"*\" ~/Git/msa-generator/data/scPDB/ ~/Git/protein-binding-site-prediction/data/scPDB/pssm/\n",
    "# Now, let us preprocess the files again to generate the features directly that can be imported into pytorch easily\n",
    "\n",
    "# USING CONCATENATION STRATEGY\n",
    "\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# List of amino acids and their integer representation\n",
    "AA_ID_DICT = {\n",
    "    \"X\": 0,\n",
    "    \"A\": 1,\n",
    "    \"C\": 2,\n",
    "    \"D\": 3,\n",
    "    \"E\": 4,\n",
    "    \"F\": 5,\n",
    "    \"G\": 6,\n",
    "    \"H\": 7,\n",
    "    \"I\": 8,\n",
    "    \"K\": 9,\n",
    "    \"L\": 10,\n",
    "    \"M\": 11,\n",
    "    \"N\": 12,\n",
    "    \"P\": 13,\n",
    "    \"Q\": 14,\n",
    "    \"R\": 15,\n",
    "    \"S\": 16,\n",
    "    \"T\": 17,\n",
    "    \"V\": 18,\n",
    "    \"W\": 19,\n",
    "    \"Y\": 20,\n",
    "}\n",
    "AA_ID_DICT = defaultdict(lambda: 0, AA_ID_DICT)\n",
    "\n",
    "# One-hot encoding and positional encoding\n",
    "feat_vec_len = 21\n",
    "feat_vec_len += 1\n",
    "\n",
    "# We generated PSSMs for select sequences\n",
    "# Mapping different sequences to the one for which we generated\n",
    "common_pssms = defaultdict(str)\n",
    "with open(os.path.join(data_dir, \"unique\"), \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split()\n",
    "        pssm_generated_for = line[0][:-1]\n",
    "        common_pssms[pssm_generated_for] = pssm_generated_for\n",
    "        for pdb_id_struct_chain in line[1:]:\n",
    "            pdb_id_struct_chain = pdb_id_struct_chain[:-1]\n",
    "            common_pssms[pdb_id_struct_chain] = pssm_generated_for\n",
    "\n",
    "\n",
    "def get_pssm(pdb_id_struct, chain_id, length):\n",
    "    pssm_pdb_id_struct, pssm_chain_id = common_pssms[\n",
    "        pdb_id_struct + \"/\" + chain_id\n",
    "    ].split(\"/\")\n",
    "    with open(\n",
    "        os.path.join(pssm_dir, pssm_pdb_id_struct, pssm_chain_id + \".pssm\"), \"r\"\n",
    "    ) as f:\n",
    "        lines = f.readlines()\n",
    "    feature = np.zeros((21, length))\n",
    "    for i, line in enumerate(lines):\n",
    "        feature[i] = np.array(line.strip().split(), dtype=np.float32)\n",
    "    return feature\n",
    "\n",
    "\n",
    "# PSSM length\n",
    "feat_vec_len += 21\n",
    "\n",
    "# Amino acid physico-chemical features selected by removing highly correlated features\n",
    "AA_sel_feats = get_amino_acid_properties(\n",
    "    os.path.join(data_dir, \"selected_features.csv\")\n",
    ")\n",
    "feat_vec_len += len(AA_sel_feats[\"X\"])\n",
    "\n",
    "\n",
    "def generate_input(sample):\n",
    "    \"\"\"\n",
    "    Generate input for a single sample which is a dictionary containing required items\n",
    "    \"\"\"\n",
    "    X = np.zeros((feat_vec_len, sample[\"length\"]))\n",
    "\n",
    "    # One-hot encoding\n",
    "    X[:21] = np.array([np.eye(21)[AA_ID_DICT[el]] for el in sample[\"sequence\"]]).T\n",
    "\n",
    "    # PSSM\n",
    "    X[22:43] = sample[\"pssm\"]\n",
    "\n",
    "    # AA Properties\n",
    "    X[43:] = np.array([AA_sel_feats[aa] for aa in sample[\"sequence\"]]).T\n",
    "\n",
    "    # Invert the distance map and matrix multiply with X so that we get a combination of all features\n",
    "    inverted_dist = 1 / sample[\"ca_dist_map_true\"]\n",
    "    np.fill_diagonal(inverted_dist, 1.0)\n",
    "    X_T = inverted_dist.dot(X.T)\n",
    "    for i in range(sample[\"length\"]):\n",
    "        X_T[i] = X_T[i] / np.sum(inverted_dist[i])\n",
    "\n",
    "    X = X_T.T\n",
    "\n",
    "    # Positional encoding\n",
    "    X[21] = np.arange(1, sample[\"length\"] + 1, dtype=np.float32) / sample[\"length\"]\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "for pdb_id_struct in sorted(os.listdir(preprocessed_dir)):\n",
    "    flg = True\n",
    "    pre = os.path.join(preprocessed_dir, pdb_id_struct)\n",
    "    features_file = os.path.join(pre, \"features.npy\")\n",
    "    labels_file = os.path.join(pre, \"labels.npy\")\n",
    "\n",
    "    if os.path.exists(labels_file):\n",
    "        continue\n",
    "    print(pdb_id_struct)\n",
    "\n",
    "    for file in sorted(os.listdir(pre)):\n",
    "        # In case features were generated but not labels, redo it\n",
    "        if file == \"features.npy\":\n",
    "            continue\n",
    "        chain_id = file[-len(\".npy\") - 1 : -len(\".npy\")]\n",
    "        sample = np.load(os.path.join(pre, file))\n",
    "        sample = {\n",
    "            key: sample[key].item() if sample[key].shape is () else sample[key]\n",
    "            for key in sample\n",
    "        }\n",
    "        sample[\"pssm\"] = get_pssm(pdb_id_struct, chain_id, sample[\"length\"])\n",
    "        if flg:\n",
    "            X = generate_input(sample)\n",
    "            y = sample[\"labels\"]\n",
    "            flg = False\n",
    "        else:\n",
    "            # Using concatenation strategy\n",
    "            tmp = generate_input(sample)\n",
    "            X = np.concatenate((X, tmp), 1)\n",
    "            y = np.concatenate((y, sample[\"labels\"]), 0)\n",
    "\n",
    "    np.save(features_file, X)\n",
    "    np.save(labels_file, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us save the above files and store in a safe space\n",
    "\n",
    "\n",
    "def archive_dir(folder, pattern, name):\n",
    "    parent_dir = os.path.dirname(folder)\n",
    "    folder = os.path.basename(folder)\n",
    "    #     print(parent_dir, folder)\n",
    "    if os.path.exists(os.path.join(parent_dir, name)):\n",
    "        print(\"Warning:\", name, \"already exists in\", parent_dir)\n",
    "        inp = input(\"Do you want to overwrite existing file? (y/n): \")\n",
    "        if inp[0].lower() == \"n\":\n",
    "            return\n",
    "    # Using only a single ! command since multiple ! spawn different bash shells\n",
    "    !cd $parent_dir; find $folder -name \"$pattern\" | tar --sort=name -I zstd -cf $name -T -; rsync -avP $name crvineeth97@ada:/share2/crvineeth97/compressed/; cd -\n",
    "    # To untar, use\n",
    "    # !tar -I zstd -xf $name\n",
    "\n",
    "\n",
    "archive_dir(raw_dir, \"*\", \"raw.tar.zst\")\n",
    "archive_dir(preprocessed_dir, \"*.npy\", \"features_labels.tar.zst\")\n",
    "archive_dir(preprocessed_dir, \"*.npz\", \"preprocessed_chains.tar.zst\")\n",
    "archive_dir(pssm_dir, \"*\", \"pssm.tar.zst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISSUES\n",
    "\n",
    "<input type=\"checkbox\"> Improve downloading PDB and fasta by using something that resumes downloads\n",
    "\n",
    "<input type=\"checkbox\"> Not sure if checking for a space in the residue is the best way of checking. Can use the code_with_modified_residues dictionary from NWalign.py (https://zhanglab.ccmb.med.umich.edu/NW-align/NWalign.py) (Should be OK) - Not Ok, it's missing some RNA sequences that start with an \"X\"\n",
    "\n",
    "<input type=\"checkbox\"> Make the pdb_id field in the preprocessed files into pdb_id_struct\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
