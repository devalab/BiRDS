{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitskorchcondae55dff3dc5444e92a7d51787ddcd9901",
   "display_name": "Python 3.7.6 64-bit ('skorch': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part of the code is used for msa_generation\n",
    "# An scPDB.tar.gz file is already present in the repo and it contains sequence.fasta and downloaded.pdb files for all the scPDB data points\n",
    "# Let us extract that into data/scPDB\n",
    "# Most of the code is taken from Deepmsa https://zhanglab.ccmb.med.umich.edu/DeepMSA/ with some small changes\n",
    "!tar xvzf scPDB.tar.gz -C ./data/\n",
    "# Download uniref50.fasta and uniclust30_2017_10 into data folder\n",
    "!aria2c -c -x 8 -s 8 -d \"./data/\" ftp://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref50/uniref50.fasta.gz\n",
    "!aria2c -c -x 8 -s 8 -d \"./data/\" http://wwwuser.gwdg.de/~compbiol/uniclust/2017_10/uniclust30_2017_10_hhsuite.tar.gz\n",
    "!tar xvzf ./data/uniref50.fasta.gz -C ./data/\n",
    "!tar xvzf ./data/uniclust30_2017_10_hhsuite.tar.gz -C ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some important constants\n",
    "# ENSURE THAT YOU RUN THIS CODE CELL FIRST\n",
    "import os\n",
    "\n",
    "data_dir = os.path.abspath(\"./data\")\n",
    "raw_dir = os.path.join(data_dir, \"scPDB\")\n",
    "splits_dir = \"./splits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to generate MSAs for the protein sequences in the dataset\n",
    "# For that, we need to split the sequence.fasta file into respective chain.fasta files\n",
    "# Also, we need to remove the fasta files of DNA/RNA seqeuences\n",
    "\n",
    "for file in sorted(os.listdir(raw_dir)):\n",
    "    file = file.strip()\n",
    "    pre = os.path.join(raw_dir, file)\n",
    "    \n",
    "    # Read SEQRES entries in PDB file to determine whether a chain\n",
    "    # has a protein sequence or not\n",
    "    pdb_file = os.path.join(pre, \"downloaded.pdb\")\n",
    "    do_not_include = set()\n",
    "    with open(pdb_file, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line[:6] != \"SEQRES\":\n",
    "            line = f.readline()\n",
    "        while line[:6] == \"SEQRES\":\n",
    "            chain_id = line[11]\n",
    "            residue = line[19:22]\n",
    "            # Generally DNA/RNA have 1 or 2-letter codes\n",
    "            if \" \" in residue:\n",
    "                do_not_include.add(chain_id)\n",
    "            line = f.readline()\n",
    "    \n",
    "    fasta = os.path.join(pre, \"sequence.fasta\")\n",
    "    with open(fasta, \"r\") as f:\n",
    "        header = f.readline()\n",
    "        while 1:\n",
    "            chain_id = header[6:7]\n",
    "            sequence = \"\"\n",
    "            line = f.readline()\n",
    "            while line != \"\" and line is not None and line[0] != \">\":\n",
    "                sequence += line.strip()\n",
    "                line = f.readline()\n",
    "            if chain_id not in do_not_include:\n",
    "                with open(os.path.join(pre, chain_id + \".fasta\"), \"w\") as hlp:\n",
    "                    hlp.write(header)\n",
    "                    hlp.write(sequence + \"\\n\")\n",
    "            if line == \"\" or line is None:\n",
    "                break\n",
    "            header = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you want to delete the generated fasta files from the above cell, use this\n",
    "# for file in sorted(os.listdir(raw_dir)):\n",
    "#     for fasta in glob(os.path.join(raw_dir, file.strip(), \"?.fasta\")):\n",
    "#         os.remove(fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fasta files generated will have a lot of common sequences\n",
    "# To speed up MSA generation, let us create a unique file that has common sequences\n",
    "# Then we can generate the MSAs for only the first chain in every line\n",
    "from collections import defaultdict\n",
    "\n",
    "sequences = defaultdict(list)\n",
    "for file in sorted(os.listdir(raw_dir)):\n",
    "    pre = os.path.join(raw_dir, file.strip())\n",
    "    for fasta in sorted(os.listdir(pre)):\n",
    "        if fasta[2:] != \"fasta\":\n",
    "            continue\n",
    "        chain_id = fasta[0]\n",
    "        with open(os.path.join(pre, fasta)) as f:\n",
    "            f.readline()\n",
    "            sequence = f.readline().strip()\n",
    "            # This choice was made so that rsync would work much better and easier\n",
    "            sequences[sequence].append(file + \"/\" + chain_id + \"*\")\n",
    "\n",
    "keys = list(sequences.keys())\n",
    "\n",
    "with open(os.path.join(splits_dir, \"unique\"), \"w\") as f:\n",
    "    for key in keys:\n",
    "        line = \"\"\n",
    "        for chain_id in sequences[key]:\n",
    "            line += chain_id + \" \"\n",
    "        f.write(line[:-1] + \"\\n\")\n",
    "\n",
    "# Let us split the MSAs into a 100 files so that they can all be run parallely\n",
    "unique_ln = len(keys)\n",
    "num_of_splits = 100\n",
    "for i in range(0, unique_ln // num_of_splits + 1):\n",
    "    with open(os.path.join(folder, str(i)), \"w\") as f:\n",
    "        for key in keys[i * num_of_splits : min(unique_ln, (i + 1) * num_of_splits)]:\n",
    "            f.write(sequences[key][0] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that boost regex library has been installed\n",
    "# MSAs take a lot of time to generate. Assuming a SLURM Workload Manager on the cluster, pssm.sh and calculate_pssm.py have been written. Make changes accordingly to make it work\n",
    "# Run pssm.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}