{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants that will be required\n",
    "# ALWAYS RUN THIS CODE CELL\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "\n",
    "data_dir = os.path.abspath(\"./data/scPDB\")\n",
    "splits_dir = os.path.join(data_dir, \"splits\")\n",
    "preprocessed_dir = os.path.join(data_dir, \"preprocessed\")\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that preprocessing has been completed from preprocessing folder\n",
    "# We define a dataset based on the preprocessed data\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Kalasanty(Dataset):\n",
    "    def __init__(self, precompute_class_weights=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.train_folds = []\n",
    "        self.valid_folds = []\n",
    "        for i in range(10):\n",
    "            with open(os.path.join(splits_dir, \"train_ids_fold\" + str(i))) as f:\n",
    "                self.train_folds.append([line.strip() for line in f.readlines()])\n",
    "        self.dataset_list = set(self.train_folds[0]).union(set(self.train_folds[1]))\n",
    "        for i in range(10):\n",
    "            self.valid_folds.append(list(self.dataset_list - set(self.train_folds[i])))\n",
    "        self.dataset = self.get_dataset()\n",
    "        self.dataset_list = sorted(list(self.dataset_list))\n",
    "        self.dataset_id_to_index = defaultdict(int)\n",
    "        for i, val in enumerate(self.dataset_list):\n",
    "            self.dataset_id_to_index[val] = i\n",
    "        if precompute_class_weights:\n",
    "            # NEED TO IMPLEMENT THIS\n",
    "            pass\n",
    "\n",
    "    def get_dataset(self):\n",
    "        available = defaultdict(list)\n",
    "        for file in os.listdir(preprocessed_dir):\n",
    "            available[file[:4]].append(file)\n",
    "\n",
    "        extras = [\"scPDB_blacklist.txt\", \"scPDB_leakage.txt\"]\n",
    "        for file in extras:\n",
    "            with open(os.path.join(splits_dir, file)) as f:\n",
    "                for line in f.readlines():\n",
    "                    line = line.strip()\n",
    "                    if line in available[line[:4]]:\n",
    "                        available[line[:4]].remove(line)\n",
    "                    if available[line[:4]] == list():\n",
    "                        del available[line[:4]]\n",
    "\n",
    "        for key in set(available.keys()) - self.dataset_list:\n",
    "            del available[key]\n",
    "\n",
    "        return available\n",
    "\n",
    "    def custom_cv(self):\n",
    "        for i in range(10):\n",
    "            train_indices = [self.dataset_id_to_index[el] for el in self.train_folds[i]]\n",
    "            valid_indices = [self.dataset_id_to_index[el] for el in self.valid_folds[i]]\n",
    "            # yield train_indices[:100], valid_indices[:12]\n",
    "            yield train_indices, valid_indices\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pdb_id = self.dataset_list[index]\n",
    "        # Just taking the first available structure for a pdb #TODO\n",
    "        pdb_id_struct = self.dataset[pdb_id][0]\n",
    "        # print(pdb_id_struct)\n",
    "        X = torch.from_numpy(\n",
    "            np.load(os.path.join(preprocessed_dir, pdb_id_struct, \"features.npy\"))\n",
    "        )\n",
    "        y = torch.from_numpy(\n",
    "            np.load(os.path.join(preprocessed_dir, pdb_id_struct, \"labels.npy\"))\n",
    "        )\n",
    "        # print(X.shape, y.shape)\n",
    "        return X, y\n",
    "\n",
    "\n",
    "# A collate function to merge samples into a minibatch, will be used by DataLoader\n",
    "def collate_fn(samples):\n",
    "    # samples is a list of (X, y) of size MINIBATCH_SIZE\n",
    "    # Sort the samples in decreasing order of their length\n",
    "    # x[1] will be y of each sample\n",
    "    samples.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    batch_size = len(samples)\n",
    "    lengths = [0] * batch_size\n",
    "    max_len = samples[0][0].shape[1]\n",
    "    X = torch.zeros(batch_size, feat_vec_len, max_len, device=device)\n",
    "    y = torch.zeros(batch_size, max_len, device=device)\n",
    "    for i, sample in enumerate(samples):\n",
    "        lengths[i] = len(sample[1])\n",
    "        X[i, :, : lengths[i]] = sample[0]\n",
    "        y[i, : lengths[i]] = sample[1]\n",
    "    return X, y, lengths\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the loss function\n",
    "from statistics import mean\n",
    "\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits as loss_func\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def compute_loss(output, target, lengths, pos_weight=None):\n",
    "    batch_size = len(lengths)\n",
    "    loss = 0.0\n",
    "    if pos_weight is None:\n",
    "        # Use a variable criterion function\n",
    "        ones = 0.0\n",
    "        zeros = 0.0\n",
    "        for i in range(batch_size):\n",
    "            one = (target[i] == 1).float().sum()\n",
    "            zeros += lengths[i] - one\n",
    "            ones += one\n",
    "        pos_weight = torch.zeros(1, device=device, dtype=torch.float32)\n",
    "        # To avoid division by 0 which should not occur. Should check dataset\n",
    "        pos_weight[0] = (zeros + 1) / (ones + 1)\n",
    "    for i in range(batch_size):\n",
    "        loss += loss_func(\n",
    "            output[i, : lengths[i]], target[i, : lengths[i]], pos_weight=pos_weight\n",
    "        )\n",
    "    return loss / batch_size\n",
    "\n",
    "\n",
    "# Define the main training loop\n",
    "pr = 1000\n",
    "\n",
    "\n",
    "def train_loop(model, dl):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(dl)\n",
    "    for i, batch_el in enumerate(pbar):\n",
    "        X, y, lengths = batch_el\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X, lengths)\n",
    "        loss = compute_loss(y_pred, y, lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({\"train_loss\": loss.item()})\n",
    "        if i % pr == pr - 1:\n",
    "            print(\"Train loss:\", running_loss / (i + 1))\n",
    "    print(\"Epoch train loss:\", running_loss / len(dl))\n",
    "\n",
    "\n",
    "# Define the main validation loop\n",
    "def valid_loop(model, dl):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(dl)\n",
    "        for i, batch_el in enumerate(pbar):\n",
    "            X, y, lengths = batch_el\n",
    "            y_pred = model(X, lengths)\n",
    "            loss = compute_loss(y_pred, y, lengths)\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix({\"valid_loss\": loss.item()})\n",
    "    print(\"Validation loss:\", running_loss / len(dl))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "from models import *\n",
    "\n",
    "max_epochs = 10\n",
    "learning_rate = 0.01\n",
    "dataset = Kalasanty()\n",
    "feat_vec_len = dataset[0][0].shape[0]\n",
    "models = []\n",
    "optimizers = []\n",
    "\n",
    "for i, (train_indices, valid_indices) in enumerate(dataset.custom_cv()):\n",
    "    model = BiLSTM(feat_vec_len).to(device)\n",
    "    print(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    models.append(model)\n",
    "    optimizers.append(optimizer)\n",
    "    print()\n",
    "    print(\"Model #\" + str(i + 1), \"--------------------------------------------\")\n",
    "    for epoch in range(max_epochs):\n",
    "        # Don't use multiprocessing here since our dataloading is I/O bound and not CPU\n",
    "        train_dl = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=1,\n",
    "            sampler=SubsetRandomSampler(train_indices),\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "        train_loop(model, train_dl)\n",
    "        valid_dl = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=1,\n",
    "            sampler=SubsetRandomSampler(valid_indices),\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "        valid_loop(model, valid_dl)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}